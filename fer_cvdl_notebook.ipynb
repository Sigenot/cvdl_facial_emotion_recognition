{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abac05a4",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fa24f",
   "metadata": {},
   "source": [
    "The goal of the project is to develop a deep learning model that can accurately classify facial expressions into one of six categories: Angry, Disgust, Fear, Happy, Sad, and Surprise. To fulfill the scope of the project we separated the neutral expressions. We will use convolutional neural networks (CNNs) to train our model, as they have been shown to be effective in image classification tasks. Our approach involves training a CNN model from scratch using the FER-2013 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3856976",
   "metadata": {},
   "source": [
    "## Data\n",
    "The FER-2013 dataset consists of 48x48 pixel grayscale images of faces that have been centered and that occupy a similar amount of space in each image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32232b",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e9c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form of training data: (23744, 64, 64, 1) (23744, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# path to folders\n",
    "dataset_path = 'dataset'\n",
    "\n",
    "# emotions and respective folders\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']\n",
    "\n",
    "def load_img_and_lbl(dataset_type):\n",
    "    faces = []\n",
    "    labels = []\n",
    "    # iterating through file\n",
    "    for emotion_index, emotion in enumerate(emotions):\n",
    "        emotion_path = os.path.join(dataset_path, dataset_type, emotion)\n",
    "        for img_name in os.listdir(emotion_path):\n",
    "            img_path = os.path.join(emotion_path, img_name)\n",
    "            \n",
    "            # load image and convert to grayscale (designed redundantly)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (64,64))\n",
    "            \n",
    "            faces.append(img)\n",
    "            labels.append(emotion_index)\n",
    "            \n",
    "    # convert lists to numpyArrays\n",
    "    faces = np.asarray(faces)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # normalizing images\n",
    "    faces = faces / 255.0\n",
    "\n",
    "    # convert labels\n",
    "    labels = to_categorical(labels, num_classes=len(emotions))\n",
    "    \n",
    "    # reshape for modell\n",
    "    faces = faces.reshape(faces.shape[0], 64, 64, 1)\n",
    "    \n",
    "    return faces, labels\n",
    "\n",
    "# load training- and testdata\n",
    "x_train, y_train = load_img_and_lbl('train')\n",
    "x_test, y_test = load_img_and_lbl('test')\n",
    "\n",
    "print(\"Form of training data:\", x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "from typing import Any, Callable, Optional, Union\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils import check_integrity, verify_str_arg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
